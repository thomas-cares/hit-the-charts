{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize distribution\n",
    "# interface\n",
    "# pass playlist get return for every son in playlist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORT and standard tweaking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spotipy\n",
    "import spotipy.util as util\n",
    "from spotipy.oauth2 import SpotifyClientCredentials\n",
    "from spotipy.oauth2 import SpotifyOAuth\n",
    "import spotipy.oauth2 as oauth2\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import os\n",
    "%matplotlib inline\n",
    "\n",
    "#visualization option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_client_id=os.getenv('spoti_CLIENT_ID')\n",
    "my_client_secret=os.getenv('spoti_CLIENT_SECRET')\n",
    "sp=spotipy.Spotify(client_credentials_manager=SpotifyClientCredentials(client_id=my_client_id, client_secret=my_client_secret))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display all columns\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "viral=pd.read_csv(r'data/analysis/viral_charts_songs_2017-2020.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Position', 'Track Name', 'Artist', 'URL', 'filename', 'ID', 'date',\n",
       "       'country', 'group', 'duration_ms', 'popularity', 'acousticness',\n",
       "       'danceability', 'energy', 'key', 'instrumentalness', 'liveness',\n",
       "       'loudness', 'mode', 'speechiness', 'valence', 'tempo', 'loud_start'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "viral.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6007"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(viral.ID.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "remarks on the features\n",
    "* 'valence' indicates the 'happiness' the lower the score the sader the music. [refercence](https://community.spotify.com/t5/Content-Questions/Valence-as-a-measure-of-happiness/td-p/4385221)\n",
    "* mode 0/1 indicates major/minor (dur/moll) 1=Major / 0=Minor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename column length to duration to match the kaggle dataframe\n",
    "viral=viral.rename(columns={'length':'duration_ms'})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### dtypes tweaking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Position              int64\n",
       "Track Name           object\n",
       "Artist               object\n",
       "URL                  object\n",
       "filename             object\n",
       "ID                   object\n",
       "date                 object\n",
       "country              object\n",
       "group                object\n",
       "duration_ms           int64\n",
       "popularity            int64\n",
       "acousticness        float64\n",
       "danceability        float64\n",
       "energy              float64\n",
       "key                   int64\n",
       "instrumentalness    float64\n",
       "liveness            float64\n",
       "loudness            float64\n",
       "mode                  int64\n",
       "speechiness         float64\n",
       "valence             float64\n",
       "tempo               float64\n",
       "loud_start          float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "viral.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#date to datetime\n",
    "viral.date=pd.to_datetime(viral.date, format=\"%Y-%m-%d\", yearfirst=True)\n",
    "\n",
    "# mode and key are categroical values\n",
    "\n",
    "#make mode categorical and name 'Major', 'Minor'\n",
    "viral['mode'] = np.where(viral['mode'].apply(str)=='1','Major','Minor')\n",
    "\n",
    "#make key categorical and label it more readable\n",
    "viral['key'] = 'key_'+viral['key'].apply(str)\n",
    "\n",
    "#introduce numeric column if viral or not 0/1\n",
    "viral['viral']=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping country codes to real country names\n",
    "keys=list(sorted(set(viral.country)))\n",
    "values=['Argentina','Australia','Brazil','Canada','Germany','Egypt','Spain','United Kingdom','Indonesia','India','Iceland','Italy','Japan','Marocco','Mexico','New Zealand','Russia','Turkey','Ukraine','United States of America','South Africa']\n",
    "country_dict = dict(zip(keys, values))\n",
    "viral['country']=viral['country'].map(country_dict) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13699"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(viral)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CAN WE PREDICT IF SONGS ARE VIRAL SONGS\n",
    "* Supervised learning model with K NEAREST NEIGHBORS\n",
    "* Will the musical features of snongs labeled as viral be distinguishable enough?\n",
    "* Using a large kaggle song data base as baseline to compare against songs from viral charts\n",
    "* https://www.kaggle.com/yamaerenay/spotify-dataset-19212020-160k-tracks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### only want to keep unique songs: 'top50_unique'\n",
    "* drop date, country, Position, filename, popularity so we can drop songs that are in the data set multiple times\n",
    "* they arent important for the analysis at hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6007"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(viral.ID.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "viral_unique=viral.drop(['Position','filename','date','country'],axis=1).drop_duplicates(ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6007"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(viral_unique)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set Up Spotify dataset from Kaggle as baseline to compare against"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle=pd.read_csv(r'data/kaggle/data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding column indicating they are not the viral songs\n",
    "kaggle['group']='base'\n",
    "\n",
    "kaggle['viral']=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mode and key are categroical values\n",
    "\n",
    "#make mode categorical and name 'Major', 'Minor'\n",
    "kaggle['mode'] = np.where(kaggle['mode'].apply(str)=='1','Major','Minor')\n",
    "\n",
    "#make key categorical and label it more readable\n",
    "kaggle['key'] = 'key_'+kaggle['key'].apply(str)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### subsetting for years 2016-2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'base' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-d976e282eb06>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# only want to keep songs from 2016 to 2020 to avoid accidentally measuring change in music production\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mkaggle_year\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkaggle\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkaggle\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'year'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2019\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2020\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2021\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'# of records: '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m#sadly reissues of old songs cannot be filterd out (ex. Frank Sinatra and Joni Mitchel)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'base' is not defined"
     ]
    }
   ],
   "source": [
    "# only want to keep songs from 2016 to 2020 to avoid accidentally measuring change in music production\n",
    "kaggle_year=kaggle[kaggle['year'].isin([2019,2020,2021])].reset_index(drop=True)\n",
    "print('# of records: ',len(base))\n",
    "#sadly reissues of old songs cannot be filterd out (ex. Frank Sinatra and Joni Mitchel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### are there any viral charts songs in the kaggle data set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base=kaggle_year[-kaggle_year['id'].isin(viral.ID.unique())]\n",
    "len(base)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SET UP FOR MODELLING\n",
    "* create joint dataframe for analysis\n",
    "* check multicollinearity\n",
    "* check outliers\n",
    "* create dummies for categorical variable (ex key & mode)\n",
    "* standardize continuous variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create joint dataframe \n",
    "create joint df of viral charts songs and songs from kaggle DB I want to compare against"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of relevant columns\n",
    "continuous_feat=[\n",
    "            \"acousticness\",\n",
    "            \"danceability\",\n",
    "            \"duration_ms\",\n",
    "            \"energy\",\n",
    "            \"instrumentalness\",\n",
    "            \"liveness\",\n",
    "            \"loudness\",\n",
    "            \"speechiness\",\n",
    "            \"tempo\",\n",
    "            \"valence\"]\n",
    "\n",
    "categorical_feat=[\"key\",\"mode\"]\n",
    "\n",
    "target_feat=['viral']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DATA FRAME ORIGINAL VALUES\n",
    "\n",
    "#### joint dataframe with true values \n",
    "named: analysis_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concat viral DF with DF from kaggle that i use as base\n",
    "analysis_true=pd.concat([viral[target_feat+categorical_feat+continuous_feat],\n",
    "                        base[target_feat+categorical_feat+continuous_feat]\n",
    "                        ],\n",
    "                       axis=0,\n",
    "                        ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DATA FRAME: SCALED AND STANDARDIZED\n",
    "#### joint dataframe with standardized continuous variables\n",
    "named: analysis_stan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit without transform for later use when applying to new data \n",
    "# copy=False parameter overrites the values in the data frame instead of jus returning the standardized data\n",
    "scaler = StandardScaler(copy=False).fit(analysis_true[continuous_feat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_stan=analysis_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#standardize the contnuous feature only\n",
    "analysis_stan[continuous_feat]=pd.DataFrame(StandardScaler(copy=False).fit_transform(analysis_true[continuous_feat]),columns=continuous_feat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### checking multicollinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "corr = round(analysis_stan.corr(),3)\n",
    "f, ax = plt.subplots(figsize=(10, 10))\n",
    "sns.set_style(\"darkgrid\")\n",
    "mask = np.triu(np.ones_like(corr, dtype=bool)) # removing the other side of the heatmap\n",
    "cmap = sns.diverging_palette(230, 20, as_cmap=True) #preparing cmap\n",
    "sns.heatmap(corr,mask=mask,cmap=cmap,linewidths=.5,square=True,annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create dummies of categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "#picking only the categroical vairbles : key and mode\n",
    "cat = analysis_stan[categorical_feat]\n",
    "enc = OneHotEncoder()\n",
    "cat_encoded = pd.DataFrame(enc.fit_transform(cat).toarray(), columns = enc.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.concat((analysis_stan[continuous_feat], cat_encoded), axis=1)\n",
    "y=analysis_stan['viral']\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split test and train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K NEAREST NEIGHBOUR\n",
    "* distance based algorithms demand that all variables need to be standardized\n",
    "* they all need to be on the same scale "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model and predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knear_model = KNeighborsClassifier(n_neighbors=3).fit(X_train, y_train)\n",
    "knear_pred = knear_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate k nearest neighbour w/ score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(knear_model.score(X_test, y_test))\n",
    "print(pd.Series(knear_pred).value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate k nearest neighbours w/ precision, recall and f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate k nearest neighbour: eval_knear\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "eval_knear={'Precision': precision_score(y_test, knear_pred, pos_label=1),#knear_pred predicitons of k nearest neighbour\n",
    "            'Recall': recall_score(y_test, knear_pred, pos_label=1),\n",
    "            'F1-Score': f1_score(y_test, knear_pred, pos_label=1)}\n",
    "\n",
    "pd.DataFrame(eval_knear,index=[0]).style.bar(color='lightblue', vmin=0.0, vmax=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate k nearest neighbours w/ Confusion Matrix and heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cf_matrix = confusion_matrix(y_test, knear_pred)#knear_pred predicitons of k nearest neighbour\n",
    "print(cf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "cmap = sns.diverging_palette(230, 20, as_cmap=True) #preparing cmap\n",
    "sns.set(font_scale=1.2)\n",
    "sns.heatmap(cf_matrix/np.sum(cf_matrix), annot=True, \n",
    "            fmt='.2%', cmap=cmap,xticklabels=[ 'predicted random song', 'predicted viral song'],\n",
    "            yticklabels=['actually a random song','actually a viral song']).set_title('K NEAREST NEIGHBOUR: CONFUSION MATRIX')\n",
    "plt.yticks(rotation = 0)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DECISION TREE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dt_model = DecisionTreeClassifier().fit(X_train, y_train)\n",
    "dt_pred = dt_model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate decision tree : eval_dt\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "eval_dt={'Precision': precision_score(y_test, dt_pred, pos_label=1),\n",
    "            'Recall': recall_score(y_test, dt_pred, pos_label=1),\n",
    "            'F1-Score': f1_score(y_test, dt_pred, pos_label=1)}\n",
    "\n",
    "pd.DataFrame(eval_dt,index=[0]).style.bar(color='lightblue', vmin=0.0, vmax=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_matrix = confusion_matrix(y_test, dt_pred) #dt_pred : predictions decision tree\n",
    "print('confusion matrix: \\n',cf_matrix)\n",
    "f, ax = plt.subplots(figsize=(15, 10))\n",
    "cmap = sns.diverging_palette(230, 20, as_cmap=True) #preparing cmap\n",
    "\n",
    "akws = {'ha': 'left','va': 'top'}\n",
    "\n",
    "ax=sns.heatmap(cf_matrix/np.sum(cf_matrix), annot=True, annot_kws=akws,\n",
    "            fmt='.2%', cmap=cmap,xticklabels=[ 'predicted random song', 'predicted viral song'],\n",
    "            yticklabels=['actually a random song','actually a viral song'])\n",
    "plt.yticks(rotation = 0)\n",
    "plt.title('DECISION TREE: CONFUSION MATRIX', size=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature importance\n",
    "in decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature importance embedded in y axis\n",
    "f, ax = plt.subplots(figsize=(25, 8))\n",
    "ax = sns.barplot(x=X.columns, \n",
    "                 y=dt_model.feature_importances_).set_xticklabels(ax.get_xticklabels(), \n",
    "                          rotation=45, \n",
    "                          horizontalalignment='right', size = 15)\n",
    "plt.title('Decision Tree: Feature Importance', size=20)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(dict(zip(X.columns, dt_model.feature_importances_)),index=[0]).T.rename(columns={0:'importance'}).sort_values(by='importance',ascending=False).head().style.bar(color='lightblue',vmin=0.0, vmax=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RANDOM FOREST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "forest_model = RandomForestClassifier().fit(X_train, y_train)\n",
    "forest_pred = forest_model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate decision tree : eval_dt\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "eval_forest={'Precision': precision_score(y_test, forest_pred, pos_label=1),\n",
    "            'Recall': recall_score(y_test, forest_pred, pos_label=1),\n",
    "            'F1-Score': f1_score(y_test, forest_pred, pos_label=1)}\n",
    "\n",
    "pd.DataFrame(eval_forest,index=[0]).style.bar(color='lightblue', vmin=0.0, vmax=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_matrix = confusion_matrix(y_test, forest_pred) #dforest_pred : predictions forest tree\n",
    "f, ax = plt.subplots(figsize=(15, 10))\n",
    "cmap = sns.diverging_palette(230, 20, as_cmap=True) #preparing cmap\n",
    "\n",
    "akws = {'ha': 'left','va': 'top'}\n",
    "\n",
    "ax=sns.heatmap(cf_matrix/np.sum(cf_matrix), annot=True, annot_kws=akws,\n",
    "            fmt='.2%', cmap=cmap,xticklabels=[ 'predicted random song', 'predicted viral song'],\n",
    "            yticklabels=['actually a random song','actually a viral song'])\n",
    "plt.yticks(rotation = 0)\n",
    "plt.title('Forest Tree : CONFUSION MATRIX', size=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature importance\n",
    "in decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature importance embedded in y axis\n",
    "f, ax = plt.subplots(figsize=(25, 8))\n",
    "ax = sns.barplot(x=X.columns, \n",
    "                 y=forest_model.feature_importances_).set_xticklabels(ax.get_xticklabels(), \n",
    "                          rotation=45, \n",
    "                          horizontalalignment='right', size = 15)\n",
    "plt.title('Forest Tree: Feature Importance', size=20)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM SUPPORT VECTOR MACHINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_model = SVC(probability=True).fit(X_train, y_train) # to return propability of classification rather than prediction\n",
    "svm_pred = svm_model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(svm_model.score(X_test, y_test))\n",
    "print(pd.Series(svm_pred).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "eval_forest={'Precision': precision_score(y_test, forest_pred, pos_label=1),\n",
    "            'Recall': recall_score(y_test, svm_pred, pos_label=1),\n",
    "            'F1-Score': f1_score(y_test, svm_pred, pos_label=1)}\n",
    "\n",
    "pd.DataFrame(eval_forest,index=[0]).style.bar(color='lightblue', vmin=0.0, vmax=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_matrix = confusion_matrix(y_test, svm_pred) #dforest_pred : predictions forest tree\n",
    "f, ax = plt.subplots(figsize=(15, 10))\n",
    "cmap = sns.diverging_palette(230, 20, as_cmap=True) #preparing cmap\n",
    "\n",
    "akws = {'ha': 'left','va': 'top'}\n",
    "\n",
    "ax=sns.heatmap(cf_matrix/np.sum(cf_matrix), annot=True, annot_kws=akws,\n",
    "            fmt='.2%', cmap=cmap,xticklabels=[ 'predicted random song', 'predicted viral song'],\n",
    "            yticklabels=['actually a random song','actually a viral song'])\n",
    "plt.yticks(rotation = 0)\n",
    "plt.title('Support Vector Machine : CONFUSION MATRIX', size=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature importance\n",
    "in decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature importance embedded in y axis\n",
    "f, ax = plt.subplots(figsize=(25, 8))\n",
    "ax = sns.barplot(x=X.columns, \n",
    "                 y=forest_model.feature_importances_).set_xticklabels(ax.get_xticklabels(), \n",
    "                          rotation=45, \n",
    "                          horizontalalignment='right', size = 15)\n",
    "plt.title('Support Vector Machine: Feature Importance', size=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  RADAR PLOT\n",
    "### FEATURE MEANS OF VIRAL SONGS VERSUS DER FEATURE MEANS OF KAGGLE SONGS\n",
    "reference: https://www.kaggle.com/typewind/draw-a-radar-chart-with-python-in-a-simple-way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=np.array(analysis_true[continuous_feat].columns)\n",
    "viral_songs_stats=analysis_true[continuous_feat][analysis_true['viral']==1].mean().values\n",
    "\n",
    "###\n",
    "base_songs_stats=analysis_true[continuous_feat][analysis_true['viral']==0].mean().values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "angles=np.linspace(0, 2*np.pi, len(labels), endpoint=False)\n",
    "angles=np.concatenate((angles,[angles[0]]))\n",
    "###\n",
    "viral_songs_stats=np.concatenate((viral_songs_stats,[viral_songs_stats[0]]))\n",
    "base_songs_stats=np.concatenate((base_songs_stats,[base_songs_stats[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=(10, 10))\n",
    "ax = fig.add_subplot(111, polar=True)\n",
    "plot1=ax.plot(angles, base_songs_stats, '.', linewidth=2, label='base songs',color='blue')\n",
    "plot2=ax.plot(angles, viral_songs_stats, '.', linewidth=2, label='viral songs',color='red')\n",
    "ax.fill(angles, base_songs_stats, alpha=0.25, color='blue')\n",
    "ax.fill(angles, viral_songs_stats, alpha=0.25, color='red')\n",
    "ax.set_thetagrids((angles * 180/np.pi)[0:10], labels)\n",
    "plt.legend(title='Legend', loc='upper left')\n",
    "ax.set_title('AVERAGE\\nVIRAL SONGS vs BASELINE SONGS\\n\\n',size=20)\n",
    "ax.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RUN PREDICTOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spotipy\n",
    "import spotipy.util as util\n",
    "from spotipy.oauth2 import SpotifyClientCredentials\n",
    "from spotipy.oauth2 import SpotifyOAuth\n",
    "import spotipy.oauth2 as oauth2\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_clickable(val):\n",
    "    return '<a href=\"{}\">{}</a>'.format(val,val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_hit(url,continuous_feat,categorical_feat):\n",
    "    \n",
    "    ###\n",
    "    \n",
    "    my_client_id=os.getenv('spoti_CLIENT_ID')\n",
    "    my_client_secret=os.getenv('spoti_CLIENT_SECRET')\n",
    "    sp=spotipy.Spotify(client_credentials_manager=SpotifyClientCredentials(client_id=my_client_id, client_secret=my_client_secret))\n",
    "\n",
    "    ###\n",
    "    \n",
    "    meta_feat=['ID','Track_Name','Artist','Audio']\n",
    "    \n",
    "    #set up to call spotify API\n",
    "    song_id=re.findall(r'\\d\\w+',str(url))[0]\n",
    "    \n",
    "    meta = sp.track(song_id)\n",
    "    features = sp.audio_features(song_id)\n",
    "    analysis= sp.audio_analysis(song_id)\n",
    "    \n",
    "    #create dictionary\n",
    "    keys=[i for i in meta_feat+continuous_feat+categorical_feat] #these lists should contain the nemes of all features I need\n",
    "    values=[song_id,\n",
    "            meta['name'],#song title\n",
    "            meta['artists'][0]['name'],#artist name\n",
    "            meta['preview_url'], #audio sinppet 30secs\n",
    "            features[0]['acousticness'],\n",
    "            features[0]['danceability'],\n",
    "            meta['duration_ms'],\n",
    "            features[0]['energy'],\n",
    "            features[0]['instrumentalness'],\n",
    "            features[0]['liveness'],\n",
    "            features[0]['loudness'],\n",
    "            features[0]['speechiness'],\n",
    "            features[0]['tempo'],\n",
    "            features[0]['valence'],\n",
    "            features[0]['key'],\n",
    "            features[0]['mode']]\n",
    "\n",
    "    df=pd.DataFrame(dict(zip(keys,values)),index=[0])\n",
    "    \n",
    "    #standardize the contnuous feature only: using the scaler i fitted with all continuouse variables\n",
    "    df[continuous_feat]=pd.DataFrame(scaler.transform(df[continuous_feat]),columns=continuous_feat)\n",
    "    \n",
    "    #recreate dummies for keys\n",
    "    for i in range(0,12):\n",
    "        if df['key'][0] == i:\n",
    "            df[f'x0_key_{i}']=1.0\n",
    "        else:\n",
    "            df[f'x0_key_{i}']=0.0\n",
    "    df.drop('key',axis=1,inplace=True)\n",
    "    \n",
    "    #recreate dummies for keys\n",
    "    df['x1_Major']=np.where(df['mode'] == 1,1.0,0.0)\n",
    "    df['x1_Minor']=np.where(df['mode'] == 0,1.0,0.0)\n",
    "    df.drop('mode',axis=1, inplace=True)\n",
    "\n",
    "    X_NEW=df.drop(['ID','Track_Name','Artist','Audio'],axis=1)\n",
    "    \n",
    "    #MAKING PREDICTION\n",
    "    dt_result=dt_model.predict(X_NEW)\n",
    "    knear_result=round(knear_model.predict_proba(X_NEW)[0][1],2)\n",
    "    forest_probability=round(forest_model.predict_proba(X_NEW)[0][1],2) #probablitiy positiv\n",
    "    svm_probability=round(svm_model.predict_proba(X_NEW)[0][1],2)\n",
    "    \n",
    "    #RADAR PLOT OF GIVEN SONGS\n",
    "    \n",
    "    labels=np.array(continuous_feat)\n",
    "    stats=df.loc[0,labels].values\n",
    "    \n",
    "    angles=np.linspace(0, 2*np.pi, len(labels), endpoint=False)\n",
    "    # close the plot\n",
    "    stats=np.concatenate((stats,[stats[0]]))\n",
    "    angles=np.concatenate((angles,[angles[0]]))\n",
    "\n",
    "    fig=plt.figure(figsize=(8, 8))\n",
    "    ax = fig.add_subplot(111, polar=True)\n",
    "    ax.plot(angles, stats, 'o-', linewidth=1, color='white')\n",
    "    ax.fill(angles, stats, alpha=0.25,color='red')\n",
    "    ax.set_thetagrids((angles * 180/np.pi)[0:10], labels)\n",
    "    ax.set_title(f\"{meta['artists'][0]['name']} - {meta['name']}\")\n",
    "    ax.grid(True)\n",
    "    \n",
    "    \n",
    "    #RETURN\n",
    "    results_dic={'Artist':df['Artist'][0],\n",
    "                 'Song':df['Track_Name'][0],\n",
    "                 'DecisionTree': np.where(dt_result==1,'VIRAL','FAILURE'),\n",
    "                 'K-Near':f'{round(knear_result*100,2)}%',\n",
    "                 'RandForest':f'{round(forest_probability*100,2)}%',\n",
    "                 'SVM-Model':f'{round(svm_probability*100,2)}%',\n",
    "                 'Audio':df['Audio'][0],\n",
    "                }\n",
    "       \n",
    "    print('\\n------')\n",
    "    return pd.DataFrame(results_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paste URL of Spotify Song\n",
      "Should look like this: https://open.spotify.com/track/7qzftATxwdp8G78aMmizXN\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-c446214bd8b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Paste URL of Spotify Song\\nShould look like this: https://open.spotify.com/track/7qzftATxwdp8G78aMmizXN'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mpredict_hit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcontinuous_feat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcategorical_feat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/kittycat/lib/python3.8/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    859\u001b[0m             )\n\u001b[0;32m--> 860\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    861\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/kittycat/lib/python3.8/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    903\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 904\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    905\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    906\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "print('Paste URL of Spotify Song\\nShould look like this: https://open.spotify.com/track/7qzftATxwdp8G78aMmizXN')\n",
    "url=input().strip(' ')\n",
    "predict_hit(url,continuous_feat,categorical_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Songs predicted a viral\n",
    "\n",
    "US viral charts: https://open.spotify.com/playlist/37i9dQZEVXbKuaTI1Z1Afx\n",
    "\n",
    "ambiguise\n",
    "* https://open.spotify.com/track/4pBhTGnL5N5KqsyqU58jee&si=48fdc4e4ae8b4d25\n",
    "* https://open.spotify.com/track/5mHrHnIqsPoB5eek6FcYfm&si=c88657fa9a684d3c\n",
    "* Lil Pump https://open.spotify.com/track/43ZyHQITOjhciSUUNPVRHc?si=61YOoQNmRZeTQaeTlEqArg\n",
    "---\n",
    "winner\n",
    "* currently in german viral charts https://open.spotify.com/track/5zPT86unoFaNP2J348n1p0&si=3f34e27a90584902\n",
    "* Vossi Bop Stormzy https://open.spotify.com/track/5DXKtoZLm31msT7tNGNHLG&si=74197ab0140b4678\n",
    "* Cradle: https://open.spotify.com/track/18A7ha5BitZjmdHTCwXFbU\n",
    "* Airwaves https://open.spotify.com/track/5CqkgDH8QZjSqqI3HmYxDD&si=e7b286fbe9cd46d9\n",
    "* Capital Bra https://open.spotify.com/track/0Dlcu2fgHklOnajVu2dUNU&si=d7f89487035e4e63\n",
    "* Apache Fame https://open.spotify.com/track/0QjjaCaXE45mvhCnV3C0TA&si=31f9ea9d3c2443bf\n",
    "* The Weekend https://open.spotify.com/track/0VjIjW4GlUZAMYd2vXMi3b&si=593d03f738ad4d58\n",
    "* Perfume Geniuse https://open.spotify.com/track/6imeB9iHkhAj0Q5sAAANB3&si=6cd7dd154b2c46b8\n",
    "\n",
    "funny losers\n",
    "* Love will tear us apart https://open.spotify.com/track/34iOH7LY3vme5rQxsVILZ4&si=051da9cc0c324f8b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# World in Sounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n",
    "world=world.rename(columns={'name':'country'})\n",
    "world.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#harmonize country names between world df and viral df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " list(viral.country) in list(world.country)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "world.country.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viral.country.unique()\n",
    "#sorted(list(viral.country.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge on country\n",
    "viral_map=world.merge(viral, on='country', how='right')\n",
    "viral_map.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mapping variables\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable #to adapt size of legend\n",
    "\n",
    "fig, ax = plt.subplots(1,1,figsize=(20,15))\n",
    "\n",
    "divider = make_axes_locatable(ax)\n",
    "cax = divider.append_axes(\"right\", size=\"5%\", pad=0.1)\n",
    "viral_map.plot(ax=ax, column='danceability',cmap='Blues',legend=True,cax=cax)\n",
    "ax.set(xlabel='Longitude(Degrees)',ylabel='Latitude(Degrees)',title='Viral Map: Danceability')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/asimislam/tutorial-python-subplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Numerical Data\n",
    "#continuous_feat\n",
    "\n",
    "#  plot Numerical Data\n",
    "a = 5  # number of rows\n",
    "b = 2  # number of columns\n",
    "c = 1  # initialize plot counter\n",
    "\n",
    "fig = plt.figure(figsize=(14,8))\n",
    "\n",
    "for i in continuous_feat:\n",
    "    ax=plt.subplot(a, b, c)\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.1)\n",
    "    viral_map.plot(ax=ax, column=i,cmap='Blues',legend=True,cax=cax)\n",
    "    ax.set(xlabel='Longitude(Degrees)',ylabel='Latitude(Degrees)',title=f'Viral Map: {i}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PLOTS\n",
    "https://seaborn.pydata.org/tutorial/distributions.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"    \n",
    "    if result==1:\n",
    "        return print(f\"YEAH! THE COMMITTEE OF MEME CATS APPROVES!\\n„{(df['Track_Name'][0])}“ by {df['Artist'][0].upper()} will go viral\\n------\\n„{(df['Track_Name'][0])}“ by {df['Artist'][0].upper()} meets {round(probability*100,2)}% of this model's viral qualities\")\n",
    "    else:\n",
    "        return print(f\"NOPE! The song „{(df['Track_Name'][0])}“ doesn't have what it takes - {df['Artist'][0].upper()} ain't on the meme-team\\n------\\n„{(df['Track_Name'][0])}“ by {df['Artist'][0].upper()} only meets this model's viral qualities by {round(probability*100,2)}%\")\n",
    "    \n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fun",
   "language": "python",
   "name": "fun"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
